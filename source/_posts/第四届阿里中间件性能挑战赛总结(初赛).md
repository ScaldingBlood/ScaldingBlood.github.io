---
title: 第四届阿里中间件性能挑战赛总结(初赛)
date: 2018-09-13 19:01:45
tags:
---

### Intro
![](logo.png)
上个学期抽空打了一下[阿里中间件的比赛](https://tianchi.aliyun.com/programming/introduction.htm?spm=5176.100066.0.0.6acd33afBcahZs&raceId=231657)。作为少见的程序设计竞赛，还是有些收获的。最后的成绩如下：

![](result.png)
总体说起来马马虎虎吧，感觉和排行榜顶尖选手的差距还是很大的。后文也会说到他们的技术思路。

### 题意分析
赛题详情见[这里](https://code.aliyun.com/middlewarerace2018/docs?spm=5176.11409106.555.1.627f6668H7cbDv)
题目描述非常长，而且复杂。这是第一个难关，要有耐心好好审题，理解要做什么。这里再简单描述下：
目标是实现一个Service Mesh，也就是在不改变服务本身的情况下，通过添加以 proxy 或 sidecar 形式部署的 Agent，实现服务的治理（服务注册、负载均衡等）。通过这种方式，所有进出服务的流量都会被Agent拦截，因此可以做到协议的转换，够使得基于不同技术框架和通讯协议建设的服务也可以实现互联互通。
![service-mesh-architecture](service-mesh-architecture.png)

系统架构如下：
![system-architecture](system-architecture.png)
图中所有的服务都运行在Docker容器中。流程是 Consumer 通过 Consumer-Agent 在 etcd 注册服务，通过Dubbo协议进行RPC。Provider 会通过 Provider-Agent 在 etcd 中查找已经注册的 Consumer 服务并向其 Agent 发送RPC的请求。此外，Provider 还向外界暴露RPC调用接口提供调用。Consumer 的调用要做到负载均衡。以上，需要达到最高的qps。

初赛最nice的地方是提供了一个Demo，让我们这些没见过上面很多技术的人可以大致先了解一下。事实上，我的大部分工作最开始都是基于这份demo。

[Agent示例](https://code.aliyun.com/middlewarerace2018/agent-demo)
[Provider 及 Consumer 服务](<https://code.aliyun.com/middlewarerace2018/services)

其中官方提供的 Provider 和 Consumer 是不可以被修改的。它们都是基于Spring-boot的，逻辑较为简单。其中 Provider 提供了Dubbo的RPC服务，暴露了一个IHelloService接口：
```java
public interface IHelloService {
    int hash(String str) throws Exception;
}
```
实现的hash方法如下
```java
@Override
public int hash(String str) throws Exception {
    int hashCode = str.hashCode();
    logger.info(++count + "_" + hashCode);
    sleep(50);
    return hashCode;
}
```
每次调用线程睡眠50ms，模拟运行消耗。该服务需要使用Dubbo协议调用，除此以外不需要其他的Dubbo知识。

Consumer 就像一个常见的Spring-boot服务，提供了一个控制器。随机生成了1024长度字符串。使用AsyncHttpClient异步的向 Provider-Agent 进行RPC的请求，得到返回值并和该字符串的hash值做比较。如果相同则返回值为OK的ResponseEntity，不同则返回值为ERROR的ResponseEntity。示例中还用到了SpringMVC中的异步返回，也就是返回DeferredResult实例。当值被赋给该实例时才会返回。此外AsyncHttpClient库提供了发送异步请求而不阻塞线程的方式，可以学习一下它的API，很实用。具体的函数如下：
```java
@RequestMapping(value = "/invoke")
public DeferredResult<ResponseEntity> invoke() {

    String str = RandomStringUtils.random(r.nextInt(1024), true, true);

    String url = "http://127.0.0.1:20000";

    DeferredResult<ResponseEntity> result = new DeferredResult<>();

    org.asynchttpclient.Request request = org.asynchttpclient.Dsl.post(url)
            .addFormParam("interface", "com.alibaba.dubbo.performance.demo.provider.IHelloService")
            .addFormParam("method", "hash")
            .addFormParam("parameterTypesString", "Ljava/lang/String;")
            .addFormParam("parameter", str)
            .build();

    ListenableFuture<org.asynchttpclient.Response> responseFuture = asyncHttpClient.executeRequest(request);

    Runnable callback = () -> {
        try {
            // 检查返回值是否正确,如果不正确返回500。有以下原因可能导致返回值不对:
            // 1. agent解析dubbo返回数据不对
            // 2. agent没有把request和dubbo的response对应起来
            String value = responseFuture.get().getResponseBody();
            if (String.valueOf(str.hashCode()).equals(value)){
                result.setResult(ok);
            } else {
                result.setResult(error);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    };
    responseFuture.addListener(callback, null);

    return result;
}
```

至此，题意也基本清晰了。我们需要实现的是两个Agent即 Consumer-Agent 和 Provider-Agent。其中 Provider-Agent 负责接收 Provider 的请求，并通过 etcd 查找 Consumer 服务，从而向 Consumer-Agent 发送请求并接收返回，再将结果返回给Provider。 而 Consumer-Agent 负责向 etcd 注册，并接收来自 Provider-Agent 的请求，并向 Consumer 请求结果并返回，再返回给 Provider-Agent。
题目页面上有这么一个展示流程的表格：
<table><thead><tr><th>通讯环节</th><th>序列化协议</th><th>远程通讯协议</th><th>备注</th></tr></thead><tbody><tr><td>Client =&gt; Consumer</td><td>（无参数传递）</td><td>HTTP</td><td></td></tr><tr><td>Consumer =&gt; Consumer Aagent</td><td>FORM</td><td>HTTP</td><td></td></tr><tr><td>Consumer Agent =&gt; Provider Agent</td><td>FORM</td><td>HTTP</td><td>可根据需要自定义</td></tr><tr><td>Provider Agent =&gt; Provider</td><td>JSON</td><td>DUBBO</td><td></td></tr><tr><td>Provider =&gt; Provider Agent</td><td>JSON</td><td>DUBBO</td><td></td></tr><tr><td>Provider Agent =&gt; Consumer Agent</td><td>TEXT</td><td>HTTP</td><td>可以根据需要自定义</td></tr><tr><td>Consumer Agent =&gt; Consumer</td><td>TEXT</td><td>HTTP</td><td></td></tr><tr><td>Consumer =&gt; Client</td><td>TEXT</td><td>HTTP</td><td></td></tr></tbody></table>

题目中还有一个设定，总共有三个能力不同的 Provider 提供Dubbo的RPC服务，分别为small medium large。因此。我们也需要提供分别对应的 Provider-Agent。Provider 能力不同体现在占用系统资源百分比不同，比例为1:2:3。

### 环境搭建
[官方配置教程](https://code.aliyun.com/middlewarerace2018/benchmarker)
比赛所用到的环境都是基于Docker的，因为整体环境较为复杂而且需要做到能够在本地评测分数，当时搭建环境也花了一个晚上，所以这里也稍微介绍一下。
系统运行时存在两个角色，一个是施压机，另一个是被压机。可以简单的理解为，被压机用于运行我们的程序，而施压机用于向我们的程序提供输入并接收输出，从而计算qps做为分数。
系统简单起见，设置施压机寻找被压机的方式是，修改施压机的host文件，将被压机的域名修改为shuke.\*\*\*。\*\*\*为施压机的主机名。

###### 被压机
被压机在测试过程中，被需要被施压机启动相应的环境。因此需要在相应的用户目录下创建.passwd文件，并在其中配置密码。此外，被压机需要安装好Docker环境。
###### 施压机
施压集需要配置python运行环境以及pipenv以运行测试脚本。此外还需要配置wrk，用于压力测试。
###### 测试流程
首先启动一个mock server。它是使用python的BaseHTTPRequestHandler和HTTPServer实现的，作用是在POST请求时返回返回一个带有docker镜像路径的JSON串。
还需要配置一下测试脚本里的bootstrap.conf,具体见官方教程。

`pipenv run python bootstrap.py -p <prefix>` 启动测试。prefix为hostname前缀。

整套测试是基于自动化的python脚本，线上环境应该和线下一样。

* bootstrap
入口，读取logging.yml作为全局`logging`的配置，输出到console。事实上线上测试是写到文件的。
使用`argparse`读取命令行的`prefix`参数和配置文件名，默认为bootstrap.conf。通过`configparser`将配置文件读入`Configuration`实例`config`中。
将`config`传入`TaskAgent`。其作用主要是拉取需要执行的测试任务，因为这里是线下环境，所以我们的任务由前面提到的Mock Server提供。将一些身份信息组为payload，由`requests`发送post请求Server的url。信息和url都在bootstarp.conf中配置。返回的信息保存为一个`Task`实例，保存的主要是队伍ID，任务ID，仓库路径，镜像路径，Docker仓库的用户名和密码。（提交代码时也需要上传打包好的镜像，不过阿里云提供自动构建）本地测试的话这些都不需要。
以上基本配置完成后，将`task`和`config`传入`Workflow`开始测试流程。

* workflow
初始化中生成`Workspace`实例，包含`local`和`remote`两个成员，分别代表施压机和被压机的工作目录，并且包含属性为相应的路径。
随机生成一个五位数的salt。
    1. __lock_local_workspace()
    创建local(施压集)的目录环境。尝试创建.lock文件，成功直接返回，失败抛出异常。
    1. __generate_dockerpwd_file()
    尝试将task中的docker密码写入.dockerpwd文件，成功返回，失败抛异常。
    1. \__create_remote_task_home()
    将创建remote(被压机)目录的shell脚本写到一个字符串中，调用`__run_remote_script`执行。`__run_remote_script`主要是将脚本用ssh组合起来，之后调用`__run_script`执行。而`__run_script`是通过`subprocess.Popen`的`communicate`方法执行脚本，获取outs, errs。通过`Popen`的`returncode`获取返回值。回到原方法，如果返回值不为0，抛出异常。
    1. __lock_remote_task_home()
    和上一个方法类似，不过把shell脚本改为创建.lock文件。
    1. \__upload_dockerpwd_file()
    同样是内容为一个scp的shell脚本，不过调用的是`__run_local_script`，这个方法直接调用`__run_script`在本地执行脚本。
    1. __docker_login()
    被压机调用的是docker login的shell脚本，此外docker仓库的密码之前已经被存到文件中，读取之后删除。
    1. __pull_docker_images()
    远程执行脚本，读取之前保存在~/.passwd位置的用户密码，sudo调用docker pull拉取我们上传的镜像、etcd镜像、ncat镜像
    1.  __check_signatures()
    上传的镜像在构建时会拉取provider consumer 和 docker-entrypoint.sh，需要确保未被修改。远程执行脚本，读取.passwd以root权限调用docker run 执行sha256sum和之前算好的值比较。不一致则返回错误码，抛出异常。
    1. __create_docker_network()
    远程执行创建docker network的脚本。子网为10.10.10.0/24，网关为10.10.10.1，子网掩码为255.255.255.0。
    1.  __start_etcd()
    远程执行启动docker etcd的脚本，从config中指定etcd的资源等参数，网络为刚才创建的docker network。在TaskHome目录下创建etcd文件夹，里面创建logs文件夹保存日志文件，启动docker时映射相应位置。此外，docker run还制定了cidfile保存在etcd文件夹内。
    然后用实用ncat镜像连接刚才创建的etcd，尝试MAX_ATTEMPTS次，每次失败sleep一定时间，成功后结束，失败则返回错误码。
    1.  __start_providers(salt)
    过程同上， 不过是启动三个不同规格的provider，提供的是provider镜像需要的参数。ncat连接也是相似的过程。此外还需要将salt值传给启动的docker，但是在docker内嗯entrypoint脚本中并没有用到这个值。
    
    这里再讲解一下provider及consumer的docker镜像构建及启动过程，也就是Dockerfile。
        * 复制项目文件到workspace/agent
        * 将work/agent设为工作区
        * mvn clean package
        * 拉取provider和consumer的jar包复制到指定目录，将项目打成的jar包也复制到目标目录
        * 拉取docker-entrypoint.sh到目标位置，它的作用是启动Provider和Consumer
        * 将start-agent.sh复制到目标位置，它的作用是按照我们配置的参数启动Agent
        * 创建/root/logs为日志目录
        * 暴露8087端口
        * 将docker-entrypoint.sh设为ENTRYPOINT
    docker-entrypoint.sh及start-agent.sh都是简单的脚本文件，docker-entrypoint.sh执行结束后会执行start-agent.sh。都是`java -jar`方式启动相应的服务，日志保存在镜像里指定的位置，其他主要是type和端口的配置，还有分配的内存大小。docker-entrypoint.sh配置了三种provider和consumer的内存大小，而start-agent.sh则是需要我们手动配置的。

    1.  __start_consumer(salt)
    过程同上，启动镜像参数type为consumer。
    1.  __warmup_then_pressure()
    热身然后压力测试。本地执行shell脚本。
    ```
    wrk -t{threads} -c{connections} -d{duration} -T{timeout} \
                    --script=./benchmark/wrk.lua \
                    --latency http://{hostname}/invoke
    ```
    使用partial，传入wrk_timeout和hostname。
    对于不用的压力，使用相应的threads connections duration。本地测试脚本的线程数为2，连接数分别为64/128/256，测试60s。
    --latency会输出结果，--script制定了wrk.lua脚本，指定了done函数，规定了输出的格式，包挎QPS，平均响应时间，最大最小响应时间等。通过HTTP响应码来判断请求是否成功（200/300）。
    然后使用正则表达式从输出的结果中获取每次的QPS。
    1.  \__stop_services()
        * \__stop_consumer()
        * \__stop_providers()
        * \__stop_etcd()
    步骤类似，先从相应的目录读取cidfile，获取容器的cid。调用docker logs，将输出重定向到之前映射容器内部用于保存日志的位置。之后stop & rm
    1.  __cleanup()
        * __remove_docker_image() 使用镜像名称做docker rmi操作。我们自己打包上传的镜像名称在task里获取。
        * __unlock_remote_task_home() 删除被压机工作目录下的.lock文件
        * __unlock_local_task_home() 删除施压集工作目录下的.lock文件
    2.  __collect_data()
        * __compute_result() 计算之前得到所有QPS中的最大值作为结果
        * __download_logs() 远程执行shell脚本，叫工作目录的所有文件都打包成.tar.gz，保存在当前目录的上一级，文件名为logs。之后本地执行scp脚本将这个文件复制到本地工作目录，将teamId/taskId写入当前目录的.osspath文件。
以上基本上就是完成的工作流程，最终如果中途抛出了异常，则返回qps为-1的json串，否则用best_qps组成一个json返回。boostrap得到这个结果后用TaskAgent将结果上传到目标url。

整个流程是自动化的，当时在比赛时没有时间细看，现在赛后分析才认真学习了一下，感觉虽然功能并不复杂但还是挺厉害的，而且日志的配置很到位，错误的排查以及流程的确认都很清晰。

### Agent
现在我们回到题目本身，已经清楚的是Provider和Consumer的输入输出和程序最终的目标，需要实现的是Agent（一个Consumer-Agent和三个Provider-Agent），Agent的代码只有一份，会根据启动的参数-Dtype判断是Consumer还是Provider。
Demo中已经为我们实现了一个Agent的样例，是可以跑通的，借此也可以帮助理解Agent具体实现的功能和细节。因为代码稍微有点多，只挑重点说一下流程。
这个Agent和之前的Rrovider和Consumer类似，使用了Spring-boot来处理请求和响应。

使用`HelloController`作为程序的入口，定义了一个`EtcdRegistry`成员用于服务注册和服务发现。`EtcdRegistry`是一个工具类，使用初始化时传入ip地址。etcd是一个基于Go的高可用强一致性的服务发现存储仓库，[这里](https://linux.cn/article-4810-1.html)有一篇介绍，不展开讨论了。etcd在系统中负责服务的注册与发现并提供负载均衡的能力，完成题干中描述的两项任务。实际上我们只需对样例稍加修改就可以直接使用代码。不过etcd的负载均衡能力是基于多个etcd节点的，这里我们还是需要自己实现。
`EtcdRegistry`中主要使用了coreos的jetcdAPI，通过传入IP来创建endpiont，然后创建lease。它拥有超时时间和唯一ID。之后启动一个线程向etcd发送心跳包，保持连接。通过系统参数判断是Provider还是Consumer（Agent），如果是Provider则进行服务注册，构造键值并传入etcd。键由预定义头、服务名、IP和端口组成，值为空。如果为Consumer则会用预定义头和服务名去查找所有满足的键值对，用获取到的IP和端口构造endpoint列表返回。

回到Controller，仅提供了一个接口invoke。传入的dubboRPC所需的参数。在invoke内部会根据系统参数判断Provider和Consumer，并依此调用相应的方法。接下来按流程讲解一下（先Consumer再Provider）
* Consumer的流程：使用okhttp库向之前通过etcd查找出来的endpoint以随机选择一个的方式发送请求，并等待响应返回。
* Provider的流程：生成一个RpcClient实例，用于向Dubbo发送请求。RpcClient的invoke方法中构造了一个Request的POJO实例，设置了一些Dubbo请求首部的字段，该程序中意义不大不赘述，然后创建了一个PRCInvovation的POJO实例，把方法的参数（Dubbo的请求体）设置到该实例中。使用`DataArrayOutputStream`和JsonUtil将PPCInvocation作为JSON的byte数组设为Request的一个属性data（Provider要求）。此外Request还拥有一个成员id，使用`AtomicLong`保证自增，唯一标识一个对象。现在我们有构造好的Request实例了。
-- 接下来是示例中的重点：
首先我们应该明确的是接下来我们需要做的是像Dubbo发送RPC请求并接受结果，此时应该使用的是Dubbo协议（题目附录中有简要介绍），在这里使用了一个很重要的第三方库，Netty。在RpcClient中有一个成员ConnectManager，完成了初始化bootstarp的操作。在我们的invoke操作中从ConnectManager中获取了channel的单例（provider-agent和provider是一对一的关系），并把request写入其中。之后，我们需要获取请求结果，这里也有点意思。实例提供了一个RpcRequestHolder类，里面有一个`ConcurrentHashMap`成员并提供了get set方法，在发送请求前，创建了一个RpcFuture实例用于接收返回的结果然后把唯一的请求id作为键，RpcFuture作为值存到RpcRequestHolder中去。netty在接收到返回后会通过reponse中携带的requestID来找到对应的RpcFuture并把内容写到其中。RpcFuture中还包含了一个RpcResponse实例，等同于响应的byte数组。在发送完向channel中写入请求后，invoke方法会调用RpcFuture的get方法等待响应的结果。在RpcFuture中有一个`CountDownLatch`值为1的实例，在netty写入reponse后会将其减一。get方法会调用`CountDownLatch`的`await`方法等待它的值为0并返回结果。

至此，一套完整的流程已经结束，除了一个部分，netty处理dubbo协议的编解码，处理响应（因为这里的netty作为一个客户端）。也即是`ChannelInitializer`的内容。一共用到三个类，DubboRpcEncoder用于编码，DubboRpcDecoder用于解码，RpcClientHandler用于处理响应。
* DubboRpcEncoder 继承了`MessageToByteEncoder`类， 在`encode`方法中把message(request)写到buffer(ByteBuf)中。包括两部分：写入首部，写入消息体。过程如下：记录当前buffer的index，把buffer的index设为当前index+首部length，写入json序列化后的信息体，然后重置index并写入首部。其中JSON序列化是题目中dubbo要求的，使用了阿里的`SerializeWriter`和`JSONSerializer`序列化对象。
* DubboRpcDecoder 与encoder类似，用于解码dubbo协议，获取消息体。不过这里有个常用操作，检查buffer可读字节的长度，如果每有超过首部长度或者超过了但是没有超过整个消息的长度（通过首部内的长度字段判断），会重置`ByteBuf`读取的index。直到没有可读数据退出。解码后，会将消息体封装为`RpcResponse`，交给接下来的handler处理。
* RpcClientHandler 通过RpcResponse内的id，从之前保存RpcFuture的RpcRequestHolder中获取对应的RpcFuture，调用done方法来去除`CountDownLatch`的锁，返回dubboRPC的结果。
示例的qps线上测试的结果在900左右。

接下来的内容是我修改这个示例的过程：
###### version 1
首先想到可以修改的是Agent Consumer请求Agent Provider的OKHttp这块的内容。Consumer发送请求使用的是AsyncHttpClient，要在有限的资源限制情况下获取尽可能高的资源利用率，Agent也可以使用这种方式，这一步操作我在比赛中并没有做……我一开始就去把agent之间的通讯替换成了netty的方式。

示例中agent与consumer交互是我第一次接触netty，这种异步的交互方式刷新了我的认知，感觉这种方式很强无敌。于是我觉得使用netty来替代agent之间的spring boot会显著的提高qps。但其实并不是这样，这里还是阻塞的只是我当时还没有意识到。我做的操作就是模仿示例中netty与dubbo交互，不过channelpipline内部的内容修改了。
分为客户端和服务端简要讲解下，客户端是consumer-agent，服务端是provider-agent。客户端把使用okhttp发送请求更换为netty客户端发送请求。创建了一个ConsumerAgentClient类，因为要发送的是dubbo RPC的相关参数，而且后面在服务端会把这些参数组装为RpcInvocation，索性我直接就在客户端直接将对象封装好，之后服务端直接传递就行。这里我也和实例一样是通过RPCFuture阻塞直到数据返回（事实上这里就是性能提升的关键）…… ConsumerAgentClient会使用AgentConnectManager获取channel，AgentConnectManager管理了netty客户端的创建和管理。这里我又照抄了示例中的ConnectionManager……也就是说我的consumer-agent只会和一个provider-agent建立连接😂
不过这里我并没有直接使用实例中的ByteToMessageDecoder作decoder的基类，而是使用了LengthFieldBasedFrameDecoder。实例中provider需要不断去判断数据包的长度，这些工作完全可以由LengthFieldBasedFrameDecoder替我们完成。需要设置的参数如下：
```
MAX_FRAME_LENGTH = 4 * 1024; //最大帧长度
LENGTH_FIELD_LENGTH = 4; //长度字段长度
LENGTH_FIELD_OFFSET = 0; //长度字段距离帧头部偏移量
LENGTH_ADJUSTMENT = 0; //长度调整量（是否要去掉头部长度，取决于长度字段的计算方式）
INITIAL_BYTES_TO_STRIP = 0; // 丢弃首部的字段长度，比如解析出来没有用处的首部可以直接丢弃
```
这里decode解析的帧格式为 长度 + requestId + 内容数据。之后通过requestId去holder中获取future中断阻塞。
服务端的内容就相对简单一些，同样通过decode来解析得到客户端发送的requestId和RpcInvocation对象，去holder中按requestId存储RpcFuture。在decode之后服务端还有一个inboundhandler负责处理业务逻辑。这里就是执行之前agent-provider在controller中的逻辑。
当然最后这个方式最终跑出来的结果惨不忍睹……qps大约就100……因为整个系统阻塞的太严重，原本基于spring-boot的bio还是一个请求对应一个线程，现在全部阻塞在一个线程上了。

###### version 2
首先把consumer-agent请求provider-agent的channel由一个单例channel改为按hostIP:channel的键值对存到map中……解决了这个显而易见的bug
虽然阻塞问题没解决，不过我还是在这里做了一个小改进，在provider-agent decode之后的AgentServerHandler中，原本是执行在线程中阻塞的逻辑，我把这段逻辑封装到一个Runnable中，然后提交到一个初始化时创建好的线程池中去执行，感觉这里有点Reactor模式的意思。这种方式一定程度上提高了provider-agent的利用率，这种情况下我的qps达到了1400。

###### version 3
这个时候排行榜前100已经是5000左右的水平，我的代码有一些很重要的地方有问题，我终于意识到`CountDownLatch`就是万恶之源。之前对异步操作的理解不深，现在再回顾一下现有系统，consumer-agent中netty客户端发送请求，之后阻塞，等待服务端的响应。provider-agent的netty服务端收到请求后，在最后一个inboundhandler中使用netty客户端向provider发送请求，之后阻塞等待provider返回结果并向consumer-agent返回结果。
在异步执行的过程中，无意义的阻塞会耗费大量系统资源。因此这个版本主要解决的就是这个问题。让我们先解决后一个阻塞。首先我们不再使用RPCFuture及其内部的`CountDownLatch`，这里的思想是，我们不再等待数据的返回，而是当数据返回后回调响应的逻辑。也就是provider向provider-agent返回数据后，我们不等待这个数据的到达在返回给consumer-agent，而是直接执行返回个体consumer-agent的操作。代码中consumer-agent最后得到返回数据是在AgentServerHandler类中，我们要在这里执行返回给provider-agent的回掉操作，即在provider-agent与consumer-agent交互的channel中写入数据。因为题目中provider-agent只会对应一个consumer-agent，所以我们可以在provider-agent中保存他们建立的channel。我把这个channel放在了ChannelHolder中，保证AgentServerHandler对其的访问。接下来处理前一个阻塞，也就是consumer收到consumer-agent返回的数据，要返回给consumer。因为这里consumer-agent还是用spring-boot处理的收到http请求，因此需要一些特殊的技巧。在Spring3.0后有了异步返回数据的概念，通过`DeferredResult`实现，具体可以自行查阅资料，在controller中，直接返回`DeferredResult`实例，等到它的内容被设置，才会回掉返回数据，整个过程也是异步的不会产生阻塞。这样需要改造的阻塞操作就容易实现了，在consumer-agent中的netty客户端接受返回的AgentClientHandler中设置`DeferredResult`对象的内容就可以。这里还涉及到一个如何寻找到这个`DeferredResult`对象的问题，我的解决方案是：每次请求会用`AtomicLong`设置一个自增不重复的id来唯一标识一次请求，同时对应一个`DeferredResult`对象。每次创建`DeferredResult`对象时将其存到一个map中，键为id。之后的过程中发送请求和返回响应都携带这个id，最终就能通过这个id找到`DeferredResult`对象。
线上测试证明这种方式是行之有效的，qps首次突破4k。

###### version 4
虽然spring-boot的异步方式确实性能不差，但是如果使用一个netty服务端来用更加纯粹的异步方式处理响应，可以更加高效的利用资源。此时spring-boot已被弃用，程序运行时会根据参数启动响应的netty服务端。
这里consumer-agent中的netty服务端要处理http请求，所以channelpipeline需要做相应调整，使用了`HttpServerCodec`,`HttpObjectAggregator`。这些类可以方便的帮我们将http报文中的内容封装成`FullHttpRequest`对象，传递给我自己实现的HttpConsumerHandler。经过字符串处理可以获得所需的rpc参数值，以上完成了原本由springmvc负责的部分。这里的netty服务端还有一点不同的是他要处理的连接的数量不再是1而是客户端的数量。以前保存deferredresult实例的map，现在需要保存channel对象，使之后的consumper-agent中的netty客户端可以异步向channel中写入响应。当然编码为http报文的工作还是交给`HttpClientCodec`完成的。
这个版本的效果差不多进入200名的及格线，在配置了一下相关参数后qps可以达到5400。

###### version 5
既然整体流程已经是纯异步了，那可以提升的都是细节内容了。除了一个之前一直忽略的东西，provider-agent的负载均衡处理。在这之前，一直依靠的是轮询provider-agent来发送请求，这里我我的想法是依据当前provider-agent的负载情况来发送请求。这里我考虑的负载情况可以用每个provider-agent还没有处理完的请求数量来表示。而每次我们都给其一个全局唯一的id，之前的版本我们是使用这个id来对应consumer和consumer-agent交互使用的channel，每次处理完从map中去除。如果我们能够把这个对应的map拆分为三个，每个provider-agent拥有一个，通过统计map键的数量就可以得到当前provider-agent处理的负载。发送请求时直接把请求发送给负载最小的agent即可。（不过现在回想由于agent的处理能力资源并不相同，可能这样方式还不是最优解）
稍微讲解一下具体的实现。负责发送请求的ConsumerAgentClient类持有一个map，键为host值为map对象（键为requestId，值为channel），分别对应不同provider-agent。初始化与每个provider-agent连接时，传入对应的channel，最终传入到负责异步写给consumer响应的AgentClientHandler，再写操作完成后会从map中去除requestId。每次发送请求会先检查每个map键的数量，取最小的向其中插入requestId和channel。
最终这个版本的效果提升并没有想象的大，qps为5600。

###### version 6
其实到了这个时候我已经想不到什么改进了，做了很多失败的尝试，最终还是在有个不大不小的改进。查询了许多netty资料后，偶尔了解到有个叫做protobuf的序列化方式，比java自带的序列化方式快很多。netty的操作中有把对象写入成字节序列的，所以我想用protobuf来改进试一下。这里需要序列化的只有从consumer-agent向provider-agent发送的AgentRequest(id+RPCInvocation)对象，其他传输的都是byte数组。使用protobuf创建了一个ProtoRequest.java文件用来代表AgentRequest这个对象。在ConsumerAgentClient中sendRequest方法把创建ProtoRequest并写入channel，并把consumer-agent原本继承`MessageToByteEncoder`的CustomerAgentEncoder替换为`ProtobufVarint32LengthFieldPrepender`和`ProtobufEncoder`，并把provider-agent中的netty服务端`LengthFieldBasedFrameDecoder`及ProviderAgentDecoder替换为`ProtobufVarint32FrameDecoder`和`ProtobufDecoder`。
这就是我得出最高成绩的版本，qps5976.4，因为没有上6000逼死强迫症，又各种尝试最后还是没有能更近一步。

提升：
每个线程负责一个provider的连接。
控制dubbo的连接<200，否则dubbo快速失败。


consumer的CPU是瓶颈，减少consumer-agent的开销
consumer-agent 透传？ 不解析consumer的http协议而是做纯粹的转发

netty 调参：
线程数 CA3 PA1
leakDetectionLevel=disabled netty内存泄漏的检查
EpollEventLoopGroup/EpollServerSocketChannel
Epoll edge triggered 减少边缘触发的次数（consumerCPU达到饱和）
TCP_QUICKACK
TCP_NODELAY
RCVBUF_ALLOCATOR: FixedRecvByteBufAllocator

dalao的坑：透传希望能够做到零拷贝（splice） netty也有支持的方法spliceTo，但是linux不支持socket直接写到另一个socket，中间有pipe作为buffer（内核态）